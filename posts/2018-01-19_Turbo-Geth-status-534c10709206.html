<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Turbo-Geth status</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Turbo-Geth status</h1>
</header>
<section data-field="subtitle" class="p-summary">
Since I wrote the previous post on the Turbo-Geth, listing optimisations that I would like to do, I have been working non-stop on coding…
</section>
<section data-field="body" class="e-content">
<section name="6797" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="6e46" id="6e46" class="graf graf--h3 graf--leading graf--title">Turbo-Geth status</h3><p name="8a5f" id="8a5f" class="graf graf--p graf-after--h3">Since I wrote the previous post on the Turbo-Geth, listing optimisations that I would like to do, I have been working non-stop on coding, testing and benchmarking. Here is a quick update on how the previous optimisations worked out and what other new ideas I had since then. First, the items from the previous roadmap:</p><ol class="postList"><li name="6c31" id="6c31" class="graf graf--li graf-after--p">Reuse of stateDB object. This works really well so far. Currently it only works when the blocks are getting inserted in sequential order. When this is not the case, StateDB completely resets and trie has to be loaded again. I am planning to handle this with the idea of trie re-attachment. If you have a state trie for the current block, and you need to jump back (or forward), instead of throwing away the whole trie, we will start building the new trie, but also keep walking on the old trie simultaneously. When we reach suitable re-attachment points, the new trie nodes will simply be attached to the fragments of the old trie. This will make sure that chain reorgs would flush only small number of trie nodes.</li><li name="95c9" id="95c9" class="graf graf--li graf-after--li">Change the trie node caching from generation-based approach to node-count based approach. I have now fully implemented it and it works well. The trie nodes, apart from being in the trie, are also part of a double-linked list. They are getting added and removed to/from the list at the same time as they are created or discarded from the trie. Whenever a node gets “touched” during any trie modifications, it moves to the back of the list. When the list reaches set limit (currently I am experimenting with the limit of 4 million nodes), the excess of the list gets removed from the list, and subsequently, the removed nodes are removed form the trie too. Effectively, this is a LRU (least recently used) cache on top of the state trie. I am planning to produce graphs plotting the list size against the allocated memory to allow calibration of the node count limit according to the memory available on the machine.</li><li name="2061" id="2061" class="graf graf--li graf-after--li">Change the way the trie nodes are stored in the database, specifically what constitutes the key. This change works well, because the new layout of the state database means that any information about accounts and their storage can be accessed without the state trie. This leads to the insight that state trie is only really required to verify the correctness of the state root hashes on the newly imported blocks. This insight is going to be crucial for the optimisation that I am currently implementing, which I expect to be the most impactful of all.</li><li name="f699" id="f699" class="graf graf--li graf-after--li">Handling of chain re-orgs. This is not implemented yet, but in addition to handling chain reorgs in the database, I also figured out how to handle tries in memory during the chain re-orgs (described above in point 1).</li><li name="c263" id="c263" class="graf graf--li graf-after--li">State reads to require only 1 database lookup. This works well. I am planning to add batched reads. Currently, each read requires seek in the database, which is walking a sorted tree in some form. If the reads are batched, we can sort the keys and perform all reads in one single tree walk.</li><li name="10f8" id="10f8" class="graf graf--li graf-after--li">Pre-fetch trie expansion lookups for state modifications, in parallel. Parallel prefetches turned out to be not as beneficial as I though. First of all, according to my tests, the maximum benefit from concurrent reads on SSD (and similar results for HDD) is x2, when reading 4k blocks from 4–5 concurrent threads. Concurrency also creates unwanted challenges for transaction management. Mass read (described above at point 5) is likely to be more beneficial.</li><li name="fd73" id="fd73" class="graf graf--li graf-after--li">Pre-fetch based on transactions in the mempool. This is still planned, but will be implemented once the sync is working and is fast.</li><li name="2263" id="2263" class="graf graf--li graf-after--li">More efficient memory usage for storing the trie. I have performed the required analysis, and, as I suspected, about third of the hexary nodes in the trie only contain 2 non-empty nodes. I have not implemented this yet, it is currently not a bottleneck at all.</li><li name="1b69" id="1b69" class="graf graf--li graf-after--li">More efficient storage of trie nodes in the database. I have done required analysis, and there is indeed massive amount of repetition in the state trie database. When I analysed data from a fully synched geth node up to block 2.2m, most repeated hashes were recorded in the database 669 times. However, I will not try to optimise this, because there is something much better we could do about the disk storage space.</li></ol><h4 name="55c9" id="55c9" class="graf graf--h4 graf-after--li">What is new?</h4><p name="4f6a" id="4f6a" class="graf graf--p graf-after--h4">10. I have swapped the database from LevelDB to BoltDB, initially because I wanted to experiment with concurrent reads. Later on, this became not as important, but I like BoltDB now because it is a very small (I could review quite a large chunk of it already and I like it) and stable code base (authors of the project consider it “complete”). I also like the absence of background compaction (it has B-tree and has to insert writes into right places straight away). I started to use transactional feature (it has proper ACID transactions) to batch up database updates and only commit them once every 8 seconds or so. It worked quite well so far, but now I hit the next bottleneck which is kind of specific to BoltDB. Even if you batch up lots of writes inside a pending ACID transaction, it still performs seeks from scratch for every single write, which is massively wasteful (this has shown up as the next bottleneck in the profiler). The solution is to instead pre-sort the writes by keys and insert them all in one walk over the database. This is what I am implementing now, and that will include a small extension to BoltDB, to support mass-Put.</p><p name="6b29" id="6b29" class="graf graf--p graf-after--p">11. As described in point 3, due to changed database layout, state trie is no longer required for anything apart from checking the state root hashes for the newly imported blocks. That means 2 improvements can be made. Firstly, state trie does not have to be written into the database all that often, only as a checkpoint just in case we have to restart the node. Secondly, we do not need to keep the old state node trie around, because even for historical tracing we can still use direct prefix-based keys. So the old trie nodes can be pruned as soon as we created few checkpoints that will most likely survive any reasonably deep reorg. This will be the next to implement after the mass-Put described in point 10.</p><p name="c857" id="c857" class="graf graf--p graf-after--p">12. Going even further from point 11, I would like to test how slow is it actually to completely (or partially) rebuild the trie from map of accounts and their storages, and compute the state root hash. Imagine if we did not keep any state trie nodes in the database, only map of accounts and storage items. Then, on start-up the node would need to regenerate the current state of the trie and cache some of it in memory. I would like to know how long would this process take, if we use efficient range scans on the database.</p><h4 name="988d" id="988d" class="graf graf--h4 graf-after--p">Data</h4><p name="54b5" id="54b5" class="graf graf--p graf-after--h4">I am currently running 2 tests in the Cloud, one with SSD and another with HDD, on machines with 26Gb of memory and 4 million trie nodes allowed in the cache. If they successfully sync to the current tip of the blockchain, I will publish some graphs, even though they might not be very impresssive :)</p><p name="60e9" id="60e9" class="graf graf--p graf-after--p graf--trailing">There are some more exciting things I will share in a few days.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@akhounov" class="p-author h-card">Alexey Akhunov</a> on <a href="https://medium.com/p/534c10709206"><time class="dt-published" datetime="2018-01-19T12:05:45.261Z">January 19, 2018</time></a>.</p><p><a href="https://medium.com/@akhounov/turbo-geth-status-534c10709206" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on July 28, 2020.</p></footer></article></body></html>